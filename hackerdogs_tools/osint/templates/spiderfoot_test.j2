"""
Test suite for {{ meta.name }} tool ({{ module_name }})

Tests:
1. Standalone tool execution
2. LangChain agent integration
3. CrewAI agent integration
"""

import pytest
import json
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv(override=True)

# Add project root to path for direct execution
project_root = Path(__file__).parent.parent.parent.parent.parent.resolve()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from langchain.agents import create_agent
from langchain_core.messages import HumanMessage
from crewai import Agent, Task, Crew

from hackerdogs_tools.osint.spiderfoot_modules.sfp_{{ module_name }}_langchain import sfp_{{ module_name }}
from hackerdogs_tools.osint.spiderfoot_modules.sfp_{{ module_name }}_crewai import Sfp{{ module_class_name|replace("sfp_", "")|replace("_", "")|title }}Tool
from hackerdogs_tools.osint.tests.test_utils import get_llm_from_env, get_crewai_llm_from_env
{% if "DOMAIN_NAME" in watched_events or "INTERNET_NAME" in watched_events %}
from hackerdogs_tools.osint.test_domains import get_random_domain
{% endif %}
{% if "IP_ADDRESS" in watched_events or "IPV6_ADDRESS" in watched_events or "AFFILIATE_IPADDR" in watched_events %}
# Note: get_random_ip() not available - using local IP for testing
{% endif %}
{% if "EMAIL_ADDR" in watched_events or "EMAILADDR" in watched_events %}
from hackerdogs_tools.osint.test_identity_data import get_random_email
{% endif %}
from hackerdogs_tools.osint.tests.test_runtime_helper import create_mock_runtime
from hackerdogs_tools.osint.tests.save_json_results import save_test_result, serialize_langchain_result, serialize_crewai_result


class Test{{ module_class_name|replace("sfp_", "")|replace("_", "")|title }}Standalone:
    """Test {{ meta.name }} tool standalone execution."""
    
    def test_{{ module_name }}_standalone(self):
        """Test {{ module_name }} tool execution without agent."""
        # Use mock runtime since ToolRuntime is auto-injected in LangChain 1.x
        runtime = create_mock_runtime(state={"user_id": "test_user"})
        
        # Determine test target based on watched events
        {% if "DOMAIN_NAME" in watched_events or "INTERNET_NAME" in watched_events %}
        test_target = get_random_domain()
        {% elif "IP_ADDRESS" in watched_events or "IPV6_ADDRESS" in watched_events or "AFFILIATE_IPADDR" in watched_events %}
        test_target = "192.168.5.1"  # Use local IP for testing (safe local network)
        {% elif "EMAIL_ADDR" in watched_events or "EMAILADDR" in watched_events %}
        test_target = get_random_email()
        {% else %}
        test_target = get_random_domain()  # Default to domain
        {% endif %}
        
        # Build invoke parameters
        invoke_params = {
            "runtime": runtime,
            "target": test_target
        }
        {% for opt_name, opt_info in prepared_opts.items() %}
        {% if opt_name != "_maxthreads" and opt_name != "api_key" %}
        # {{ optdescs.get(opt_name, '') }}
        {% if opt_info.value is not none %}
        invoke_params["{{ opt_name }}"] = {{ opt_info.json_value }}
        {% endif %}
        {% endif %}
        {% endfor %}
        {% if has_api_key %}
        # API key will be retrieved from runtime.state or environment
        {% endif %}
        
        # Tools are StructuredTool objects - use invoke() method
        result = sfp_{{ module_name }}.invoke(invoke_params)
        
        # Parse result
        try:
            result_data = json.loads(result)
        except json.JSONDecodeError:
            # If not JSON, treat as error message
            result_data = {"status": "error", "message": result}
        
        # Print JSON output for verification
        print("\n" + "=" * 80)
        print("TOOL JSON OUTPUT:")
        print("=" * 80)
        print(json.dumps(result_data, indent=2))
        print("=" * 80 + "\n")
        
        # Save JSON result to file
        target_filename = test_target.replace("@", "_at_").replace(".", "_").replace("/", "_")
        result_file = save_test_result("{{ module_name }}", "standalone", result_data, target_filename)
        print(f"üìÅ JSON result saved to: {result_file}")
        
        # Assertions
        assert result is not None, "Result should not be None"
        assert isinstance(result, str), "Result should be a string"
        
        if isinstance(result_data, dict):
            assert "status" in result_data, "Result should contain 'status' field"
            assert result_data["status"] in ["success", "error"], f"Invalid status: {result_data.get('status')}"
            if result_data["status"] == "success":
                print(f"‚úÖ Tool executed successfully")
                assert "module" in result_data, "Successful result should contain 'module' field"
            else:
                print(f"‚ö†Ô∏è  Tool returned error: {result_data.get('message', 'Unknown error')}")
        else:
            print(f"‚ö†Ô∏è  Unexpected result format: {type(result_data)}")


class Test{{ module_class_name|replace("sfp_", "")|replace("_", "")|title }}LangChain:
    """Test {{ meta.name }} tool with LangChain agent."""
    
    def test_{{ module_name }}_langchain_agent(self):
        """Test {{ module_name }} tool with LangChain agent."""
        # Create agent directly (not using pytest fixtures)
        llm = get_llm_from_env()
        tools = [sfp_{{ module_name }}]
        agent = create_agent(
            model=llm,
            tools=tools,
            system_prompt="You are a cybersecurity analyst. Use the {{ meta.name }} tool for OSINT operations. IMPORTANT: Return the tool output verbatim as JSON without any additional commentary or summarization."
        )
        
        # Determine test target based on watched events
        {% if "DOMAIN_NAME" in watched_events or "INTERNET_NAME" in watched_events %}
        test_target = get_random_domain()
        {% elif "IP_ADDRESS" in watched_events or "IPV6_ADDRESS" in watched_events or "AFFILIATE_IPADDR" in watched_events %}
        test_target = "192.168.5.1"  # Use local IP for testing (safe local network)
        {% elif "EMAIL_ADDR" in watched_events or "EMAILADDR" in watched_events %}
        test_target = get_random_email()
        {% else %}
        test_target = get_random_domain()  # Default to domain
        {% endif %}
        
        # Execute query directly (agent is a runnable in LangChain 1.x)
        # ToolRuntime is automatically injected by the agent
        result = agent.invoke({
            "messages": [HumanMessage(content=f"Use {{ meta.name }} to investigate {test_target}")],
            "user_id": "test_user"
        })
        
        # Assertions
        assert result is not None, "Agent returned None"
        assert "messages" in result or "output" in result, f"Invalid agent result structure: {result}"
        
        # Extract tool output from messages
        tool_output_message = next((msg for msg in reversed(result["messages"]) if msg.type == "tool"), None)
        if tool_output_message and tool_output_message.content:
            try:
                tool_output_json = json.loads(tool_output_message.content)
                result_data = {
                    "status": "success",
                    "agent_type": "langchain",
                    "tool_output": tool_output_json,
                    "target": test_target,
                    "user_id": "test_user"
                }
                target_filename = test_target.replace("@", "_at_").replace(".", "_").replace("/", "_")
                result_file = save_test_result("{{ module_name }}", "langchain", result_data, target_filename)
                print(f"üìÅ LangChain result saved to: {result_file}")
                assert "status" in tool_output_json and tool_output_json["status"] == "success", f"LangChain tool output status is not success: {tool_output_json}"
            except json.JSONDecodeError:
                print(f"‚ö†Ô∏è  LangChain tool output was not valid JSON: {tool_output_message.content}")
                result_data = {
                    "status": "error",
                    "agent_type": "langchain",
                    "message": "Tool output was not valid JSON",
                    "raw_content": tool_output_message.content,
                    "target": test_target,
                    "user_id": "test_user"
                }
                target_filename = test_target.replace("@", "_at_").replace(".", "_").replace("/", "_")
                result_file = save_test_result("{{ module_name }}", "langchain", result_data, target_filename)
                print(f"üìÅ LangChain result saved to: {result_file}")
                assert False, f"LangChain tool output was not valid JSON: {tool_output_message.content}"
        else:
            print(f"‚ö†Ô∏è  No tool output found in LangChain result for {test_target}")
            result_data = {
                "status": "error",
                "agent_type": "langchain",
                "message": "No tool output found in agent response",
                "raw_result": serialize_langchain_result(result),
                "target": test_target,
                "user_id": "test_user"
            }
            target_filename = test_target.replace("@", "_at_").replace(".", "_").replace("/", "_")
            result_file = save_test_result("{{ module_name }}", "langchain", result_data, target_filename)
            print(f"üìÅ LangChain result saved to: {result_file}")
            assert False, f"No tool output found in LangChain result for {test_target}"
        
        print(f"‚úÖ LangChain test completed")


class Test{{ module_class_name|replace("sfp_", "")|replace("_", "")|title }}CrewAI:
    """Test {{ meta.name }} tool with CrewAI agent."""
    
    def test_{{ module_name }}_crewai_agent(self):
        """Test {{ module_name }} tool with CrewAI agent."""
        # Create agent directly (not using pytest fixtures)
        llm = get_crewai_llm_from_env()
        agent = Agent(
            role="OSINT Analyst",
            goal="Perform OSINT operations using {{ meta.name }}",
            backstory="You are an expert OSINT analyst. You must return tool output verbatim as JSON without any additional commentary, summarization, or interpretation.",
            tools=[Sfp{{ module_class_name|replace("sfp_", "")|replace("_", "")|title }}Tool()],
            llm=llm,
            verbose=True
        )
        
        # Determine test target based on watched events
        {% if "DOMAIN_NAME" in watched_events or "INTERNET_NAME" in watched_events %}
        test_target = get_random_domain()
        {% elif "IP_ADDRESS" in watched_events or "IPV6_ADDRESS" in watched_events or "AFFILIATE_IPADDR" in watched_events %}
        test_target = "192.168.5.1"  # Use local IP for testing (safe local network)
        {% elif "EMAIL_ADDR" in watched_events or "EMAILADDR" in watched_events %}
        test_target = get_random_email()
        {% else %}
        test_target = get_random_domain()  # Default to domain
        {% endif %}
        
        task = Task(
            description=f"Use {{ meta.name }} to investigate {test_target}. IMPORTANT: Return the tool output verbatim as complete JSON without any additional commentary, summarization, or interpretation.",
            agent=agent,
            expected_output="Complete raw JSON output from {{ meta.name }} tool with status, module, target, raw_response, user_id, and note fields."
        )
        
        crew = Crew(
            agents=[agent],
            tasks=[task],
            llm=llm,
            verbose=True
        )
        
        # Execute task
        result = crew.kickoff()
        
        # Assertions
        assert result is not None, "CrewAI returned None"
        
        # Parse CrewAI result
        try:
            result_data = serialize_crewai_result(result) if result else None
            if result_data and isinstance(result_data, dict):
                # Check if result contains tool output
                if "status" in result_data and result_data["status"] == "success":
                    print(f"‚úÖ CrewAI test completed successfully")
                else:
                    print(f"‚ö†Ô∏è  CrewAI result status: {result_data.get('status', 'unknown')}")
            else:
                # Try to extract JSON from result string
                result_str = str(result)
                if "status" in result_str and "success" in result_str:
                    print(f"‚úÖ CrewAI test completed (result in string format)")
                else:
                    print(f"‚ö†Ô∏è  CrewAI result format may need review")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not parse CrewAI result: {e}")
            result_data = {"raw_result": str(result)}
        
        # Save CrewAI agent result
        try:
            target_filename = test_target.replace("@", "_at_").replace(".", "_").replace("/", "_")
            result_file = save_test_result("{{ module_name }}", "crewai", result_data, target_filename)
            print(f"üìÅ CrewAI result saved to: {result_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not save CrewAI result: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"‚úÖ CrewAI test completed")


def run_all_tests():
    """Run all three test scenarios."""
    print("=" * 80)
    print(f"Running {{ meta.name }} Tool Tests ({{ module_name }})")
    print("=" * 80)
    
    # Test 1: Standalone
    print("\n1. Testing Standalone Execution...")
    try:
        test = Test{{ module_class_name|replace("sfp_", "")|replace("_", "")|title }}Standalone()
        test.test_{{ module_name }}_standalone()
        print("‚úÖ Standalone test completed")
    except Exception as e:
        print(f"‚ùå Standalone test failed: {str(e)}")
        import traceback
        traceback.print_exc()
    
    # Test 2: LangChain
    print("\n2. Testing LangChain Agent Integration...")
    try:
        test = Test{{ module_class_name|replace("sfp_", "")|replace("_", "")|title }}LangChain()
        test.test_{{ module_name }}_langchain_agent()
        print("‚úÖ LangChain test completed")
    except Exception as e:
        print(f"‚ùå LangChain test failed: {str(e)}")
        import traceback
        traceback.print_exc()
    
    # Test 3: CrewAI
    print("\n3. Testing CrewAI Agent Integration...")
    try:
        test = Test{{ module_class_name|replace("sfp_", "")|replace("_", "")|title }}CrewAI()
        test.test_{{ module_name }}_crewai_agent()
        print("‚úÖ CrewAI test completed")
    except Exception as e:
        print(f"‚ùå CrewAI test failed: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 80)
    print("All tests completed!")
    print("=" * 80)


if __name__ == "__main__":
    run_all_tests()

